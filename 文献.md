# Deformable Convolutional Networks  
Jifeng Dai∗ Haozhi Qi∗,† Yuwen Xiong∗,† Yi Li∗,† Guodong Zhang  
主要看采样和权重方程的定义  
Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. In this work, we introduce two new modules to enhance the transformation modeling capability of CNNs, namely, deformable convolution and deformable RoI pooling.可变形卷积和可变形池化。  
It adds 2D offsets to the regular grid sampling locations in the standard convolution. It enables free form deformation of the sampling grid. It is illustrated in Figure 1. The offsets are learned from the preceding feature maps, via additional convolutional layers. Thus, the deformation is conditioned on the input features in a local, dense, and adaptive manner.![image](https://user-images.githubusercontent.com/92596875/202897667-d8b4fc09-765c-4ccb-af7b-aea521effe30.png)  
二维偏移量加入常规采样位置，偏移量通过额外的卷积层从前面的特征图中学习。(c)(d) are special cases of (b), showing that the deformable convolution generalizes various transformations for scale, (anisotropic) aspect ratio and rotation.  
![image](https://user-images.githubusercontent.com/92596875/202898416-5b82b532-b706-4b8d-b2f0-f3f7750d72e8.png)  
![image](https://user-images.githubusercontent.com/92596875/202898430-e8465926-41f6-467e-a354-3dea94e18299.png)  
![image](https://user-images.githubusercontent.com/92596875/202898443-ba5efa7d-034d-41e0-84c1-5000e6b86407.png)  
![image](https://user-images.githubusercontent.com/92596875/202898460-427f6411-b4cb-482f-a20e-e57c2735d7bb.png)
![image](https://user-images.githubusercontent.com/92596875/202898709-19c4e759-b4a1-43ca-b452-ae044ab49e5a.png)  
公式1是常规卷积，R里是距中心的偏移量，（0，1）等。从2中看出给每个位置加了偏移量。q是x中的整数位置，p由于偏移量的存在是小数，公式3是双线性插值得到inputfeature在小数位置的值。  
![image](https://user-images.githubusercontent.com/92596875/202898645-fddac021-679a-489d-8f4f-79c55cf84a34.png)  
 通道数2N对应N个2D偏移。  
#  Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition  
![image](https://user-images.githubusercontent.com/92596875/203024131-94b94921-526c-4baf-9ab1-393d32343a45.png)  
时间上的label和空间上不一致，是在空间的基础上再向后延，具体算法及数值看程序  
uni-label
![image](https://user-images.githubusercontent.com/92596875/203034670-b23d8b59-dc4e-41f4-8cf5-17c5796c7815.png)  
![image](https://user-images.githubusercontent.com/92596875/203034730-1ef1d4cd-986f-41e7-8425-099fd430afec.png)  
i代表关节，j就是每个关节和其他关节的关系，有多少邻接全部加起来，包括自接。公式的前半部分邻接阵归一化  
![image](https://user-images.githubusercontent.com/92596875/203036235-d4a4e767-b851-48fa-ba04-804ae5dded09.png)  
multi--  
![image](https://user-images.githubusercontent.com/92596875/203036799-6a100b1a-db6a-4dc0-9423-2809ca61be19.png)  
前面只需要对角线和非对角线元素就可以区分，现在多分类必须使用多个A矩阵。  
![image](https://user-images.githubusercontent.com/92596875/203037265-a2867015-8b14-4da9-8754-77a007d0fdf6.png)  
![image](https://user-images.githubusercontent.com/92596875/203037347-cd99698b-b248-4004-9d03-984d4314715a.png)  
还有一个可学的边界重要性矩阵M，M和A按元素相乘后替换A，M初始化为全1矩阵。  
# SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS    
这篇有GCN的很多底层公式  
# Contrast-reconstruction Representation Learning for Self-supervised Skeleton-based Action Recognition  
**idea:与其通过不同的管道进行对比学习后交换知识，不如直接用这个视角做data augmentation**  
对比度重建表示学习，它主要由三个部分组成：Sequence Reconstructor、Contrastive Motion Learner、Information Fuser。  
Sequence Reconstructor 通过重构从骨架坐标序列中学习表示。为了增强运动的学习，对比运动学习器分别在从坐标序列和附加速度序列学习的表示之间进行对比学习。最后，在信息融合器中，我们探索了结合序列重构器和对比运动学习器的各种策略，以及建议通过基于知识蒸馏的融合策略同时捕获姿势和运动，该策略将运动学习从对比运动学习器转移到序列重建器。  
1. Intro  
从原始坐标中学习的representation倾向于静态姿势，不捕获动作的动态。所以很多动作很难区分，比如跑步和慢跑。  
**所以建议利用速度进行表示学习**  
![image](https://user-images.githubusercontent.com/92596875/203874529-739f3852-c8eb-45d1-8047-d9624c55edcb.png)
![image](https://user-images.githubusercontent.com/92596875/203874643-cc8d4827-6434-4987-99d8-0a754c1d1efb.png)  
1）对比运动学习器（CML）通过最大化速度和骨架序列的学习表示之间的相似性来提取运动动力学； 2）序列重构器（SER）通过坐标序列重构来学习姿势； 3) Information Fuser 通过知识蒸馏将 CML 和 SER 耦合在一起——CML 的查询编码器扮演教师角色，并通过施加蒸馏损失“教”SER 的学生编码器运动动力学。 因此，学生编码器的学习表示捕获了运动动态和姿势信息，然后用于评估。 CML 中的 TAP 表示时间平均池化，用于跨时间聚合信息。  
CML里两个编码器一个学动作，一个学姿势，有点东西，其实就相当于把crossclr里的motion当作一个data augmentation了。
2. 方法  
 A. Sequence Reconstructor  
 ![image](https://user-images.githubusercontent.com/92596875/203973912-befa2a0d-fcbf-4b4a-bd1a-b5f2a1d3ca65.png)  
 这是一个基于RNN的auto-encoder（GRU），同时训练decoder从第一个和最后一个动作中复原整个序列。  
 B. Contrastive Motion Learner  
 速度就是后一个时刻的位置减前一个时刻的位置。MOCO的架构，GRU作为backbone，只不过一个输入速度一个输入关节。
 然后是TAP和MLP做一个非线性转换。  
 ![image](https://user-images.githubusercontent.com/92596875/203978742-974b4294-e988-49df-a686-8ee8ebb2836c.png)
 ![image](https://user-images.githubusercontent.com/92596875/203978769-a0f9b717-8cf7-4e34-86e7-6c57ca69dba3.png)  
 C. Information Fuser  
 主要是知识蒸馏的架构，开山作：Distilling the Knowledge in a Neural Network  
 可以先训练好一个teacher网络，然后将teacher的网络的输出结果  作为student网络的目标，训练student网络，使得student网络的结果p接近q。知识蒸馏，可以将一个网络的知识转移到另一个网络，两个网络可以是同构或者异构。做法是先训练一个teacher网络，然后使用这个teacher网络的输出和数据的真实标签去训练student网络。知识蒸馏，可以用来将网络从大网络转化成一个小网络，并保留接近于大网络的性能；也可以将多个网络的学到的知识转移到一个网络中，使得单个网络的性能接近emsemble的结果。  
 ![image](https://user-images.githubusercontent.com/92596875/203984119-04279233-3b42-440e-ae55-f1b524c56c2d.png)  
 D. Network Training  
 ![image](https://user-images.githubusercontent.com/92596875/203984790-81b860a0-1a96-4e5d-ac2a-0317be02d5c0.png)  
 分为两步，第二步是个联合loss。
# How and What to Learn: Taxonomizing Self-Supervised Learning for 3D Action Recognition
1. 摘要  
两种相互竞争的方法，生成式和对比式。前者试图从隐空间中恢复输入，后者调整隐空间中表示的距离。
将自我监督动作识别的从两个维度上分类：  
![image](https://user-images.githubusercontent.com/92596875/204141913-e8272660-13bb-4de1-9edb-e3af379547ee.png)  
**发现同时优化编码器和解码器的自监督目标函数的模型优于仅优化其中一个。对比和吸引目标函数结合能最大化模型容忍度。实现了一个方法，使用“Attractive Decoder” and “Attractive + Contrastive Encoder” objective functions。**  
2. 自监督学习  
![image](https://user-images.githubusercontent.com/92596875/204142506-2093262a-191e-4ceb-aa16-1ab2802b0d8e.png)  
  2.1 目标函数  
  Attractive SSL：Slow Feature Analysis (SFA)慢特征分析，物体姿态往往随着帧的变化而缓慢变化。现在的方法是从扰动中重建：“filling-in”, the “Jigsaw”, and “colorization”,  
  ![image](https://user-images.githubusercontent.com/92596875/204239609-8958653f-3c55-41bc-bfde-25657ed530a7.png)  
  最大化真实输入与该输入之间的相似性。这里只是采用了简单的增强，也没有decoder。  
  Contrastive SSL：包含一个额外的排斥。  
  ![image](https://user-images.githubusercontent.com/92596875/204240541-f7e45161-e144-43f6-8d02-d4f311b246c9.png)  
  2.2 Encoder and Decoder supervision  
  Self-supervised Encoding：在模型或者encoder的输出上设置目标函数。
  Self-supervised Decoding：解码器从潜在空间重构输入：  
  ![image](https://user-images.githubusercontent.com/92596875/204242148-c569f407-7021-4a9e-858d-007956961b70.png)  
  ![image](https://user-images.githubusercontent.com/92596875/204242359-9539da2d-d4b0-4fc0-bd43-17e3de917cb4.png)  
  2.3 Self-Supervised learning in Action Recognition  
  两类基于骨骼的方法：输入空间（autoencoder）和输出空间（latentspace）的重建。  
3. Taxonomizing self-supervised learning  
在动作识别中SSL的最佳方法是什么？  
**Reconstruction tasks：空间仿射变换，时间变换。**  
我们分类的一个中心目标是在动作识别中比较自我监督目标函数（在 E 和 D 内）的不同组合。为此，我们在能够实现上述任何 SSL 方法的架构上进行了实验。我们从 Su 等人的最先进网络开始。它由一个 3 层编码器 E 和一个 1 层解码器 D 组成，均由门控循环单元 (GRU) 构成。为了提高分类的训练速度、收敛性和可解释性，我们将该架构简化为 1 层编码器 E 和 1 层解码器 D。每层由 512 个单元组成（与中的 1,024 个单元相比）。与具有不同归纳偏差的架构进行比较，例如图卷积网络 (GCN)，留待未来工作。 
所有的结论：  
**Decoder learning outperforms Encoder learning.**  
**Contrastive learning and Attractive learning lead to similar performance.**  
**Combining Encoder with Decoder learning improves performance.**  
**主要结论：Including the “Contrastive and Attractive Encoder” objective functions with an “Attractive Decoder” objective function yield the best performance in our taxonomy.**

# Cross-Domain Gradient Discrepancy Minimization for Unsupervised Domain Adaptation无监督域适应的跨域梯度差异最小化  
无监督域适应 (UDA) 旨在将从标记良好的源域中学到的知识推广到未标记的目标域。  
以前的方法是双分类器对抗，只关注两个分类器输出的相似性，本文提出跨域梯度差异最小化，最小化源样本和目标样本生成的梯度差异。  
梯度为目标样本的语义信息提供了线索，因此可以作为一个很好的监督来提高目标样本的准确性。为了计算目标样本的梯度信号，我们通过基于聚类的自监督学习进一步获得目标伪标签。
UDA一种流行的范例是通过最小化特定metric来减少域之间的分布差异，另一种是利用对抗性学习来学习领域不变的特征表示。  
1. intro  
现有的对抗域适应又可以通过两种方法来实现。1）应用额外的域鉴别器来区分样本是来自源域还是目标域2）具有两个分类器的网络内对抗策略。  
![image](https://user-images.githubusercontent.com/92596875/204722729-c264ff8d-bb98-4c5c-9f7b-0f02cf10e253.png)  
但以前的双分类器方法缺乏对目标样本准确性的考虑，模糊的目标样本可能会被错误的边界检测到，导致后续对抗过程中类分布不准确。  
本文方法：源样本和目标样本之间的梯度差异和准确性有关：假设有一个准确的分类器，源数据和目标数据会产生相似的梯度信号来更新分类器。  
关键思想：我们希望两个域的Loss不仅接近最终模型，而且在整个优化过程中遵循与它相似的路径。因此，我们使用梯度信号作为替代监督并使其成为分类精度的代理，从而为 UDA 产生跨域梯度差异最小化 (CGDM) 方法。 CGDM 使用源样本和目标样本之间的梯度差异作为额外的监督。此外，考虑到仅源分类器获得的伪标签可能不够准确，我们利用基于聚类的自监督方法为目标样本获得更可靠的伪标签。通过对齐梯度向量，两个域的分布可以在类别级别更好地对齐。  
主要贡献：1）我们提出了一种新方法，它显式地最小化了源样本和目标样本产生的梯度向量的差异。2）为了计算目标样本的梯度，我们采用基于聚类的策略来获得更可靠的伪标签。  
2. 相关工作  
无监督自适应：样本加权自适应；度量学习自适应；对抗学习自适应。
 2.1 对抗性学习方法：前面提到的两种范式。本文使用第二种：使用两个不同的特定于任务的分类器来对抗生成器关于两个分类器的预测差异。  
 2.2 伪标签：基于聚类的伪标签方法显示出他们的优越性并成功应用于邻域自适应。本文建议最小化基于伪标签的跨域梯度差异，以加强普通双分类器的对抗性学习。  
 ![image](https://user-images.githubusercontent.com/92596875/204791588-4e4a6202-8320-4484-bdad-c48426ac577f.png)  
 源样本和目标样本都通过生成器G和两个分类器。源样本的分类误差使用有监督损失来最小化，使用自监督减少模糊目标样本的数量。对抗性损失的极大极小过程用于检测源域支持之外的目标样本。梯度差异损失进一步用于对对齐分布。  
3. 方法  
 3.1 UDA with Bi-Classifier Adversarial Learning  
 从source distribution中抽取ns个数据以及它们的label，从target distribution中抽取nt个数据。UDA的目标是获得一个function F可以在目标域中精确分类。  
 G提取原始输入的discriminative deep features，然后两个不同的任务特定分类器F1F2分别产生预测概率：![image](https://user-images.githubusercontent.com/92596875/204852235-ed84c363-4ed9-49da-92f7-9fc35ec4bcad.png)。  
 首先审视双分类器对抗学习的标准三步原则：
  step 1. 通过最小化losscls联合训练GF1F2：  
  ![image](https://user-images.githubusercontent.com/92596875/204853079-82c1296d-64e3-4d6e-adf2-fa2bc0faca5d.png)  
  Lce是交叉熵，这里就是G提取出的两个域的features会通过两个分类器。但只用源域的数据和label构建Loss进行学习。  
  step 2. 冻结G并更新F1F2，从而最大化它们在目标域样本上输出之间的差异。  
  ![image](https://user-images.githubusercontent.com/92596875/204884762-8ea9051d-3718-4e4a-8c04-db52590559d6.png)  
  保证两个分类器在源域的精度的同时，最大化它们在目标域的差异。  
  step 3. 冻结F1F2的参数，然后更新G以最小化两个分类器之间的差异。
 多次重复上述步骤后，该模型可以通过决策边界有效地检测源域支持之外的目标样本，并在一定程度上对齐两个域的分布。 
 **不太理解中心思想**  
 3.2 Minimizing Cross-Domain Gradient Discrepancy最小化跨域梯度差异  
 上述过程的问题：仅考虑两个分类器的差异而没有其他任何约束，意思是有可能两个分类器最终都分错了。我们从另一个角度来看问题：如果我们想学习一个分类器，通过它可以正确分类来自两个域的所有样本，**那么来自源和目标的样本产生的梯度向量应该是类似的**。因此，我们引入了两个域之间的梯度相似性度量。为此，我们首先分别用 gs 和 gt 表示源和目标示例上的预期梯度。  
 ![image](https://user-images.githubusercontent.com/92596875/204898827-5bcf1290-7424-48df-acaf-6b6732b5cd5e.png)  
 我们将伪标签分配给目标样本，为了减轻可能具有不正确伪标签的模糊目标样本的噪声，我们使用基于每个目标样本的预测熵的加权分类损失来计算梯度。  
 ![image](https://user-images.githubusercontent.com/92596875/204900717-ccb4f8b6-6153-4324-8e3d-fa838c6cf2f2.png)  
 ![image](https://user-images.githubusercontent.com/92596875/204900775-9add4945-4581-497e-ba77-e40019aba60e.png)  
 其中 δ 表示 softmax 输出，E(·) 表示标准信息熵。
 我们通过梯度差异损失 LGD 在步骤 3 中更新生成器时最小化这两个梯度向量之间的差异，这里我们使用余弦相似度来表示差异
 ![image](https://user-images.githubusercontent.com/92596875/204901188-428c5b69-2d18-4a0b-9d4d-bb59aa935e0e.png)  
 3.3 Self-supervised Learning for Target Samples  
 在本文中，我们使用加权聚类策略来获得更可靠的伪标签。  
 ![image](https://user-images.githubusercontent.com/92596875/204901694-b858267b-6995-40ba-bf34-4a1dbfb022bf.png)  
 使用两个分类器的softmax输出对样本进行加权以获得第k类的centroid（质心）ck。
 ![image](https://user-images.githubusercontent.com/92596875/204907895-9369b971-7fab-4f9a-b590-dc6b29baede7.png)  
**基本思想先看到这里，后续是作者对原有UDA管道的适配**

# Revisiting Skeleton-based Action Recognition
![image](https://user-images.githubusercontent.com/92596875/205043719-9d0c6dd6-e9c1-4d78-8076-837033b535dd.png)  
本文提出了PoseConv3D，一种基于骨架的动作识别方法。与基于GCN的方法相比，依赖3D Heatmap Volumes，在学习时空特征方面更有效，对姿势估计更稳健，并且在跨数据集设置中有更好的泛化能力。
1. Intro  
GCN的缺点：1）鲁棒性：受坐标分布偏移的显著影响2）互操作性：骨架数据很难与其他模态融合3）可扩展性：GCN复杂性与视频中人数成线性比例关系。  
本文的PoseConv3D现代姿态估计器获得的 2D 姿态作为输入。2D 姿态由骨骼关节的热图堆栈表示，而不是在人体骨骼图上操作的坐标。 不同时间步长的热图将沿时间维度堆叠以形成 3D 热图体积。 PoseConv3D 然后在 3D 热图体积之上采用 3D 卷积神经网络来识别动作。  
解决了上述基于GCN方法的局限性，在多个数据集上实现了最优的性能。
2. 相关工作  
3D-CNN for RGB-based action recognition.在这项工作中，我们首先建议使用具有 3D 热图体积的 3D-CNN 作为输入，并获得基于骨架的动作识别的最新技术水平。  
GCN for skeleton-based action recognition.  
CNN for skeleton-based action recognition.CNN 的方法首先基于手动设计的变换将骨架序列建模为伪图像。一系列工作将时间维度上的热图聚合为带有颜色编码或学习模块的2D输入。  
3. 具体工作  
![image](https://user-images.githubusercontent.com/92596875/205047204-d9ca9076-7d38-4ce0-bfe0-dc36dd9ebea5.png)  
首先回顾骨架提取，随后，我们介绍了 3D Heatmap Volume，然后是 PoseConv3D 的结构设计。  
 3.1. Good Practices for Pose Extraction  
 一般情况下2D姿势的质量更好，采用topdown的姿势估计器进行提取，在估计热图的存储方面，以往文献中常将其存储为坐标三元组(x, y, c)，其中c表示热图的最大得分，(x, y)为c对应的坐标。  
 3.2. From 2D Poses to 3D Heatmap Volumes
 从视频帧中提取 2D 姿势以输入 PoseConv3D 后，我们将它们重新表示为 3D 热图体积。  
 形式上，我们将 2D 姿势表示为大小为 K × H × W 的热图，其中 K 是关节数，H 和 W 是框架的高度和宽度。  
 我们可以直接使用 Top-Down 姿态估计器生成的热图作为目标热图，它应该被零填充以匹配给定相应边界框的原始帧。如果我们只有骨架关节的坐标三元组 (xk, yk, ck)，我们可以通过组合以每个关节为中心的 K 个高斯图来获得关节热图（预测关节在每个像素发生的概率）J：  
 ![image](https://user-images.githubusercontent.com/92596875/205146074-f2422c24-6712-4155-a1a2-b785c33718eb.png)  
# MixMIM: Mixed and Masked Image Modeling for Efficient Visual Representation Learning  
## 摘要  
MixMIM：一个简单的MIM模型，适用于各种层级视觉transformer，现有的mim采用特殊的mask，大大减慢训练速度并导致训练微调不一致。  
本文采用两幅图像相互混合，然后从中恢复原本的两幅图像。
## Intro  
复述了MIM的问题，以及MAE打破了输入图像二维结构的问题。 
![image](https://user-images.githubusercontent.com/92596875/207939490-7d712cc0-cb93-4161-81a4-94568a0374a1.png)
MixMIM：一种通用性的MIM方法，同时利用了BEiT和MAE的优势并避免了局限性。  
MixMIM 采用编码器-解码器设计。 编码器处理混合图像以获得两个部分蒙版图像的隐藏表示，而解码器重建两个原始图像。  
self-attention 操作具有全局接受域，每个标记都可以轻松地与其他标记进行交互。 具体还要再看  
MixMIM 可以广泛应用于预训练不同层次的 ViT，例如 Swin Transformer、PVT 等。我们的 MixMIM 还通过修改 Swin Transformer 作为预训练和知识转移的编码器来探索轻量级架构。  
## MixMIM for Masked Image Modeling
###  A Revisit of Masked Image Modeling  
给定输入图像 x，MIM 首先将图像划分为非重叠图像块 xp，遵循 ViT [14]。 然后它采样一个随机掩码 M 来掩蔽图像块的一部分，并用特殊符号 [MASK] 填充被掩蔽的地方  
![image](https://user-images.githubusercontent.com/92596875/207948523-199a870e-6b1d-42d5-a956-8711eb896a20.png)  
这一块就是原图像和mask的混叠，蒙版图像 xp 由图像编码器处理以产生潜在表示，并利用轻量级解码器（head）根据潜在表示重建原始图像。  
MIM 计算重建图像块 yp 和原始图像块 xp 之间的均方误差 (MSE) 作为重建损失。 预训练后，解码器被丢弃，编码器用于下游视觉任务的进一步微调。
![image](https://user-images.githubusercontent.com/92596875/207967203-e474aed7-df3a-49f5-bae4-b212330d2a75.png)  
### Mixed and Masked Image Modeling (MixMIM)  
基于MIM的自监督取得了很大进展，但仍有上述的两个缺点。  
混合输入由 MixMIM 处理以同时重建原始图像。 为了更好地将学习到的多尺度表示转移到下游任务，我们引入了一个简单的分层视觉 Transformer 作为所提出的 MixMIM 的编码器。 
![image](https://user-images.githubusercontent.com/92596875/207980949-6b323e80-9a1c-45da-a80a-f1fae8143f8e.png)  
#### Mixed Training Inputs.  
混合训练图像 xpm 表示为  
![image](https://user-images.githubusercontent.com/92596875/207982587-0a3771bc-1d52-4454-baa0-429434c8569e.png)  
该设计与MAE的原理相同，但我们的方法没有拆解 2D 图像的结构，使其更灵活地适应各种视觉主干，例如 PVT和 Swin Transformer。
#### Hierarchical Vision Transformer.  
为了更好地编码多尺度表示，我们基于 Swin Transformer 构建了 MixMIM 的编码器，并引入了一些小的修改，使 MixMIM 编码器更简单但更强大。

# Swin Transformer 层级式的使用了移动窗口（swin）的vision transformer
## 摘要  
新vision transformer，可以作为CV的通用骨干网络。直接将transformer应用到视觉领域的挑战，（1）一个是尺度的问题，同一物体大小差距大。（2）图像的分辨率。  
本文采用层级式结构，自注意力在窗口内计算（大大降低序列长度），通过shifting让相邻的两个窗口有了交互。
## 引言  
常规想把transformer转移到CV领域  
![image](https://user-images.githubusercontent.com/92596875/207986039-928d352e-acd7-4b13-b719-eb643e6cde36.png)  

#LT-GAN: Self-Supervised GAN with Latent Transformation Detection
